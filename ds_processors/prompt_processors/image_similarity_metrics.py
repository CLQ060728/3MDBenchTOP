# Author: Qian Liu

from torchmetrics.image.inception import InceptionScore
from torchmetrics.image.fid import FrechetInceptionDistance
from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity
from torchmetrics.image import PeakSignalNoiseRatio
from torchmetrics.image import StructuralSimilarityIndexMeasure
from torchmetrics.multimodal.clip_score import CLIPScore
import torch


"""
Higher Inception Score indicates more varied and recognizable generated samples. 
Lower FID score means the distribution of generated data is closer to real data based on feature embeddings.
The inception score (IS) is a mathematical algorithm used to measure or determine the quality of images created by 
generative AI through a generative adversarial network (GAN). The word "inception" refers to the spark of creativity or 
initial beginning of a thought or action traditionally experienced by humans.
The score produced by the IS algorithm can range from zero (worst) to infinity (best).
"""
def compute_IS(imgs_tensor, device):
    imgs_tensor = imgs_tensor.to(device)    #torch.device("cuda", 0)
    inception = InceptionScore(compute_with_cache=False).to(device)
    
    inception.update(imgs_tensor)
    IS = inception.compute()

    return IS


"""
Fr√©chet inception distance (FID) is a metric for quantifying the realism and diversity of images generated by 
generative adversarial networks (GANs). Realistic could mean that generated images of people look like real images of people.
Unlike the earlier inception score (IS), which evaluates only the distribution of generated images, 
the FID compares the distribution of generated images with the distribution of a set of real images ("ground truth"). 
The FID metric does not completely replace the IS metric. Classifiers that achieve the best (lowest) FID score tend to have 
greater sample variety while classifiers achieving the best (highest) IS score tend to have better quality within individual 
images.
A lower FID indicates a better match between the generated images and the real images in terms of their visual quality 
and diversity.
"""
def compute_FID(imgs_real_tensor, imgs_fake_tensor, device):
    fid = FrechetInceptionDistance(reset_real_features=False, compute_with_cache=False).to(device)
    
    imgs_real_tensor = imgs_real_tensor.to(device)
    imgs_fake_tensor = imgs_fake_tensor.to(device)
    
    fid.update(imgs_real_tensor, real=True)
    fid.update(imgs_fake_tensor, real=False)
    FID = fid.compute()
    
    return FID


"""
The Learned Perceptual Image Patch Similarity (LPIPS_) calculates perceptual similarity between two images.
LPIPS essentially computes the similarity between the activations of two image patches for some pre-defined network. 
This measure has been shown to match human perception well. A low LPIPS score means that image patches are perceptual similar.
Both input image patches are expected to have shape (N, 3, H, W). The minimum size of H, W depends on the chosen backbone 
(see net_type arg).  
"""
def compute_LPIPS(imgs_real_tensor, imgs_fake_tensor, net_type, device):
    # net_type: 'vgg(16)', 'alex(net-owt)', 'squeeze'
    lpips = LearnedPerceptualImagePatchSimilarity(net_type=net_type, compute_with_cache=False).to(device) 

    # LPIPS needs the images to be in the [-1, 1] range. single or batch images imgs_real_tensor, imgs_fake_tensor both possible.
    imgs_real_tensor = imgs_real_tensor.to(device)
    imgs_real_tensor = imgs_real_tensor.type(torch.float32) * 2 / 255 - 1
    imgs_fake_tensor = imgs_fake_tensor.to(device)
    imgs_fake_tensor = imgs_fake_tensor.type(torch.float32) * 2 / 255 - 1
    
    LPIPS = lpips(imgs_real_tensor, imgs_fake_tensor)

    return LPIPS


"""
Peak signal-to-noise ratio (PSNR) is an engineering term for the ratio between the maximum possible power of a 
signal and the power of corrupting noise that affects the fidelity of its representation. Because many signals have a 
very wide dynamic range, PSNR is usually expressed as a logarithmic quantity using the decibel scale.
PSNR is commonly used to quantify reconstruction quality for images and video subject to lossy compression.
For color images with three RGB values per pixel, the definition of PSNR is the same except that the MSE is the sum over all 
squared value differences (now for each color, i.e. three times as many differences as in a monochrome image) divided by 
image size and by three. Alternately, for color images the image is converted to a different color space and PSNR is reported 
against each channel of that color space, e.g., YCbCr or HSL.
PSNR is most commonly used to measure the quality of reconstruction of lossy compression codecs (e.g., for image compression). 
The signal in this case is the original data, and the noise is the error introduced by compression. When comparing compression 
codecs, PSNR is an approximation to human perception of reconstruction quality.
Typical values for the PSNR in lossy image and video compression are between 30 and 50 dB, provided the bit depth is 8 bits, 
where higher is better. The processing quality of 12-bit images is considered high when the PSNR value is 60 dB or higher. 
For 16-bit data typical values for the PSNR are between 60 and 80 dB. Acceptable values for wireless transmission quality loss 
are considered to be about 20 dB to 25 dB.
In the absence of noise, the two images I and K are identical, and thus the MSE is zero. In this case the PSNR is infinite.
"""
def compute_PSNR(imgs_real_tensor, imgs_fake_tensor, device):
    psnr = PeakSignalNoiseRatio(compute_with_cache=False).to(device)

    imgs_real_tensor = imgs_real_tensor.to(device)
    imgs_fake_tensor = imgs_fake_tensor.to(device)
    
    PSNR = psnr(imgs_fake_tensor, imgs_real_tensor)
    
    return PSNR


"""
The structural similarity index measure (SSIM) is a method for predicting the perceived quality of digital television and 
cinematic pictures, as well as other kinds of digital images and videos. It is also used for measuring the similarity between 
two images. 
SSIM is a perception-based model that considers image degradation as perceived change in structural information, while also 
incorporating important perceptual phenomena, including both luminance masking and contrast masking terms. The difference with 
other techniques such as MSE or PSNR is that these approaches estimate absolute errors. Structural information is the idea 
that the pixels have strong inter-dependencies especially when they are spatially close. These dependencies carry important 
information about the structure of the objects in the visual scene. Luminance masking is a phenomenon whereby image distortions
(in this context) tend to be less visible in bright regions, while contrast masking is a phenomenon whereby distortions become 
less visible where there is significant activity or "texture" in the image.
This system calculates the Structural Similarity Index between 2 given images which is a value between -1 and +1. 
A value of +1 indicates that the 2 given images are very similar or the same while a value of -1 indicates the 2 given images 
are very different. Often these values are adjusted to be in the range [0, 1], e.g., in torchmetrics, where the extremes 
hold the same meaning.
"""
def compute_SSIM(imgs_real_tensor, imgs_fake_tensor, device):
    ssim = StructuralSimilarityIndexMeasure(compute_with_cache=False).to(device)

    imgs_real_tensor = imgs_real_tensor.to(device)
    imgs_real_tensor = imgs_real_tensor.type(torch.float32)
    imgs_fake_tensor = imgs_fake_tensor.to(device)
    imgs_fake_tensor = imgs_fake_tensor.type(torch.float32)
    
    SSIM = ssim(imgs_fake_tensor, imgs_real_tensor)

    return SSIM


"""
CLIP Score is a reference-free metric that can be used to evaluate the correlation between a generated caption for an 
image and the actual content of the image. It corresponds to the cosine similarity between visual CLIP embedding for an image 
and textual CLIP embedding for a caption. The score is bound between 0 and 100 and the closer to 100 the better.
prompt length only has 77 tokens. one limitation.
"""
def compute_CLIP_SCORE(prompt, img_tensor, device):
    img_tensor = img_tensor.to(device)
    metric = CLIPScore(model_name_or_path="openai/clip-vit-large-patch14", 
                       compute_with_cache=False).to(device)

    CLIP_SCORE = metric(img_tensor, prompt)
    CLIP_SCORE = CLIP_SCORE.detach()
    
    return CLIP_SCORE