{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b651aba-60c0-44f8-8cbc-68faaab6846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "llava_path = \"/home/jovyan/3MDBench/ds_processors/prompt_processors/LLaVA_NeXT/\"\n",
    "sys.path.insert(0, llava_path)\n",
    "# ~/.cache/huggingface/hub/ cached models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea10821-58b5-472b-9111-1d936abef74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /LLaVA-NeXT/llava/conversation.py change LLAMA_TOKENIZER_PATH to local llama tokenizer\n",
    "\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IGNORE_INDEX\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "pretrained = \"lmms-lab/llama3-llava-next-8b\"\n",
    "model_name = \"llava_llama3\"  # \n",
    "device = \"cuda\"\n",
    "device_map = \"auto\"\n",
    "tokenizer, model, image_processor, max_length = load_pretrained_model(pretrained, None, model_name, device_map=device_map) # Add any other thing you want to pass in llava_model_args\n",
    "\n",
    "model.eval()\n",
    "model.tie_weights()\n",
    "\n",
    "# image1 = Image.open(\"/home/jovyan/DFBench/data/IMAGEs/VISUAL_GENOME/selected/6.jpg\")\n",
    "# image1_tensor = process_images([image1], image_processor, model.config)\n",
    "# image1_tensor = [_image.to(dtype=torch.float16, device=device) for _image in image1_tensor]\n",
    "# image2 = Image.open(\"/home/jovyan/DFBench/data/IMAGEs/VISUAL_GENOME/selected/3.jpg\")\n",
    "# image2_tensor = process_images([image2], image_processor, model.config)\n",
    "# image2_tensor = [_image.to(dtype=torch.float16, device=device) for _image in image2_tensor]\n",
    "\n",
    "conv_template = \"llava_llama_3\" #  \n",
    "question = DEFAULT_IMAGE_TOKEN + \"\\nWhat is shown in this image?\"\n",
    "conv = copy.deepcopy(conv_templates[conv_template])\n",
    "conv.append_message(conv.roles[0], question)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt_question = conv.get_prompt()\n",
    "input_ids = tokenizer_image_token(prompt_question, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(device)\n",
    "\n",
    "for img_index in [3, 4]:\n",
    "    image = Image.open(f\"/home/jovyan/DFBench/data/IMAGEs/VISUAL_GENOME/selected/{img_index}.jpg\")\n",
    "    image_tensor = process_images([image], image_processor, model.config)\n",
    "    image_tensor = [_image.to(dtype=torch.float16, device=device) for _image in image_tensor]\n",
    "    print(f\"image size: {image.size}\")\n",
    "    image_sizes = [image.size]\n",
    "    \n",
    "    cont = model.generate(\n",
    "        input_ids,\n",
    "        images=image_tensor,\n",
    "        image_sizes=image_sizes,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        max_new_tokens=256,\n",
    "    )\n",
    "    text_outputs = tokenizer.batch_decode(cont, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    print(text_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b6967ea-db8b-4d47-8202-a3938a226ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b04be60c0b8419aa05e5f515b6c37ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import builtins\n",
    "builtins.LLAVA_PATH_ = \"/home/jovyan/3MDBench/ds_processors/prompt_processors/LLaVA_NeXT/\"\n",
    "\n",
    "import image_captioner as ic\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer, model, input_ids, image_processor = ic.get_llava_next_llama3_8b_model(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b788306-1e4e-4e63-a907-8ccfebba9638",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = [\"/home/jovyan/3MDBench/data/IMAGEs/VISUAL_GENOME/selected/3.jpg\",\n",
    "             \"/home/jovyan/3MDBench/data/IMAGEs/VISUAL_GENOME/selected/6.jpg\"]\n",
    "captions = ic.get_image_captions(tokenizer, model, input_ids, image_processor, img_paths, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28de1188-ab45-4aad-988b-bb2909b2a4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c552dde8-aff0-4eb9-b189-e2010e866784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  can not be used with GPU vRAM 48GB, out-of-memory\n",
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-34b-hf\")\n",
    "\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-34b-hf\", torch_dtype=torch.float16,\n",
    "                                                          low_cpu_mem_usage=True,\n",
    "                                                          attn_implementation=\"flash_attention_2\").to(\"cuda:0\")\n",
    "\n",
    "# prepare image and text prompt, using the appropriate prompt template\n",
    "\n",
    "prompt = \"<|im_start|>system\\nAnswer the questions.<|im_end|><|im_start|>user\\n<image>\\nWhat is shown in this image?\"\\\n",
    "         \"<|im_end|><|im_start|>assistant\\n\"\n",
    "image = Image.open(\"/home/jovyan/DFBench/data/IMAGEs/VISUAL_GENOME/selected/1.jpg\")\n",
    "inputs = processor(prompt, image, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "# autoregressively complete prompt\n",
    "output = model.generate(**inputs, max_new_tokens=256)\n",
    "\n",
    "print(processor.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d4196b-5ddc-462a-a65b-8e9832ff2bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch images generation\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoProcessor, LlavaNextForConditionalGeneration\n",
    "\n",
    "# Load the model in half-precision\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n",
    "\n",
    "# Get three different images\n",
    "url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n",
    "image_stop = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image_cats = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "url = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg\"\n",
    "image_snowman = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Prepare a batched prompt, where the first one is a multi-turn conversation and the second is not\n",
    "prompt = [\n",
    "    \"[INST] <image>\\nWhat is shown in this image? [/INST] There is a red stop sign in the image. [INST] <image>\\nWhat about this image? How many cats do you see [/INST]\",\n",
    "    \"[INST] <image>\\nWhat is shown in this image? [/INST]\"\n",
    "]\n",
    "\n",
    "# We can simply feed images in the order they have to be used in the text prompt\n",
    "# Each \"<image>\" token uses one image leaving the next for the subsequent \"<image>\" tokens\n",
    "inputs = processor(text=prompt, images=[image_stop, image_cats, image_snowman], padding=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(**inputs, max_new_tokens=30)\n",
    "processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa9bc70-cd55-4077-ac4f-80db7d740393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
