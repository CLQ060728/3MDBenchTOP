{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "003cf936-3bec-455e-a4bd-0602ff4770bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "import torch\n",
    "\n",
    "size = (512, 512) # - InceptionScore, FRECHET INCEPTION DISTANCE;\n",
    "img_real = Image.open(\"/home/jovyan/DFBench/data/IMAGEs/MSCOCO/selected/000000377132.jpg\")\n",
    "img_real = img_real.resize(size, resample=Image.BILINEAR)\n",
    "img_real_tensor = pil_to_tensor(img_real).type(torch.uint8)\n",
    "img_real_tensor = torch.unsqueeze(img_real_tensor, 0)\n",
    "img_real_1 = Image.open(\"/home/jovyan/DFBench/data/IMAGEs/MSCOCO/selected/000000274035.jpg\")\n",
    "img_real_1 = img_real_1.resize(size, resample=Image.BILINEAR)\n",
    "img_real_1_tensor = pil_to_tensor(img_real_1).type(torch.uint8)\n",
    "img_real_1_tensor = torch.unsqueeze(img_real_1_tensor, 0)\n",
    "imgs_real = torch.cat((img_real_tensor, img_real_1_tensor), 0)\n",
    "img_real_1 = Image.open(\"/home/jovyan/DFBench/data/IMAGEs/MSCOCO/selected/000000037552.jpg\")\n",
    "img_real_1 = img_real_1.resize(size, resample=Image.BILINEAR)\n",
    "img_real_1_tensor = pil_to_tensor(img_real_1).type(torch.uint8)\n",
    "img_real_1_tensor = torch.unsqueeze(img_real_1_tensor, 0)\n",
    "imgs_real = torch.cat((imgs_real, img_real_1_tensor), 0)\n",
    "\n",
    "img_fake = Image.open(\"/home/jovyan/DFBench/data/IMAGEs/MSCOCO/selected/000000472617.jpg\")\n",
    "img_fake = img_real.resize(size, resample=Image.BILINEAR)\n",
    "img_fake_tensor = pil_to_tensor(img_fake).type(torch.uint8)\n",
    "img_fake_tensor = torch.unsqueeze(img_fake_tensor, 0)\n",
    "img_fake_1 = Image.open(\"/home/jovyan/DFBench/data/IMAGEs/MSCOCO/selected/000000346759.jpg\")\n",
    "img_fake_1 = img_fake_1.resize(size, resample=Image.BILINEAR)\n",
    "img_fake_1_tensor = pil_to_tensor(img_fake_1).type(torch.uint8)\n",
    "img_fake_1_tensor = torch.unsqueeze(img_fake_1_tensor, 0)\n",
    "imgs_fake = torch.cat((img_fake_tensor, img_fake_1_tensor), 0)\n",
    "img_fake_1 = Image.open(\"/home/jovyan/DFBench/data/IMAGEs/MSCOCO/selected/000000158028.jpg\")\n",
    "img_fake_1 = img_fake_1.resize(size, resample=Image.BILINEAR)\n",
    "img_fake_1_tensor = pil_to_tensor(img_fake_1).type(torch.uint8)\n",
    "img_fake_1_tensor = torch.unsqueeze(img_fake_1_tensor, 0)\n",
    "imgs_fake = torch.cat((imgs_fake, img_fake_1_tensor), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78c6e6af-a540-4bd3-988d-f19d1724053d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 3, 512, 512]), torch.Size([3, 3, 512, 512]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_real.size(), imgs_fake.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8d5de43-e5fd-4f95-9c58-fbf6fee26eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1., device='cuda:0'), tensor(0., device='cuda:0'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute Inception Score\n",
    "\"\"\"\n",
    "Higher Inception Score indicates more varied and recognizable generated samples. \n",
    "Lower FID score means the distribution of generated data is closer to real data based on feature embeddings.\n",
    "The inception score (IS) is a mathematical algorithm used to measure or determine the quality of images created by \n",
    "generative AI through a generative adversarial network (GAN). The word \"inception\" refers to the spark of creativity or \n",
    "initial beginning of a thought or action traditionally experienced by humans.\n",
    "The score produced by the IS algorithm can range from zero (worst) to infinity (best).\n",
    "\"\"\"\n",
    "\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "import torch\n",
    "\n",
    "imgs = imgs.to(torch.device(\"cuda\", 0))\n",
    "inception = InceptionScore(compute_with_cache=False).to(torch.device(\"cuda\", 0))\n",
    "# generate some images\n",
    "#imgs = torch.randint(0, 255, (100, 3, 299, 299), dtype=torch.uint8)\n",
    "\n",
    "inception.update(imgs)\n",
    "inception.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65fbba11-c8f8-4063-aec9-03ab341e38c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(377.5616, device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute Frechet Inception Distance\n",
    "\"\"\"\n",
    "Fr√©chet inception distance (FID) is a metric for quantifying the realism and diversity of images generated by \n",
    "generative adversarial networks (GANs). Realistic could mean that generated images of people look like real images of people.\n",
    "Unlike the earlier inception score (IS), which evaluates only the distribution of generated images, \n",
    "the FID compares the distribution of generated images with the distribution of a set of real images (\"ground truth\"). \n",
    "The FID metric does not completely replace the IS metric. Classifiers that achieve the best (lowest) FID score tend to have \n",
    "greater sample variety while classifiers achieving the best (highest) IS score tend to have better quality within individual \n",
    "images.\n",
    "A lower FID indicates a better match between the generated images and the real images in terms of their visual quality \n",
    "and diversity.\n",
    "\"\"\"\n",
    "\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "import torch\n",
    "\n",
    "fid = FrechetInceptionDistance(reset_real_features=False, compute_with_cache=False).to(torch.device(\"cuda\", 0))\n",
    "\n",
    "# generate two slightly overlapping image intensity distributions\n",
    "# imgs_dist1 = torch.randint(0, 200, (100, 3, 299, 299), dtype=torch.uint8)\n",
    "# imgs_dist2 = torch.randint(100, 255, (100, 3, 299, 299), dtype=torch.uint8)\n",
    "\n",
    "imgs_real = imgs_real.to(torch.device(\"cuda\", 0))\n",
    "imgs_fake = imgs_fake.to(torch.device(\"cuda\", 0))\n",
    "\n",
    "fid.update(imgs_real, real=True)\n",
    "fid.update(imgs_fake, real=False)\n",
    "fid.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c7a10e-3845-4e9d-b8a3-6521899f89c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute LEARNED PERCEPTUAL IMAGE PATCH SIMILARITY\n",
    "\"\"\"\n",
    "The Learned Perceptual Image Patch Similarity (LPIPS_) calculates perceptual similarity between two images.\n",
    "LPIPS essentially computes the similarity between the activations of two image patches for some pre-defined network. \n",
    "This measure has been shown to match human perception well. A low LPIPS score means that image patches are perceptual similar.\n",
    "Both input image patches are expected to have shape (N, 3, H, W). The minimum size of H, W depends on the chosen backbone \n",
    "(see net_type arg).\n",
    "\"\"\"\n",
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
    "import torch\n",
    "\n",
    "# net_type: 'vgg(16)', 'alex(net-owt)', 'squeeze'\n",
    "lpips = LearnedPerceptualImagePatchSimilarity(net_type='squeeze', compute_with_cache=False).to(torch.device(\"cuda\", 0)) \n",
    "\n",
    "# LPIPS needs the images to be in the [-1, 1] range.\n",
    "img_real_1_tensor = img_real_1_tensor.to(torch.device(\"cuda\", 0))\n",
    "img_real_1_tensor = img_real_1_tensor.type(torch.float32) * 2 / 255 - 1\n",
    "img_fake_1_tensor = img_fake_1_tensor.to(torch.device(\"cuda\", 0))\n",
    "img_fake_1_tensor = img_fake_1_tensor.type(torch.float32) * 2 / 255 - 1\n",
    "# batch images imgs_real, imgs_fake also possible\n",
    "\n",
    "lpips(img_real_1_tensor, img_fake_1_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5a4fc6b-286e-4dc8-90fd-49fb5b78b02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(29.5404, device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute PEAK SIGNAL-TO-NOISE RATIO\n",
    "\"\"\"\n",
    "Peak signal-to-noise ratio (PSNR) is an engineering term for the ratio between the maximum possible power of a \n",
    "signal and the power of corrupting noise that affects the fidelity of its representation. Because many signals have a \n",
    "very wide dynamic range, PSNR is usually expressed as a logarithmic quantity using the decibel scale.\n",
    "PSNR is commonly used to quantify reconstruction quality for images and video subject to lossy compression.\n",
    "For color images with three RGB values per pixel, the definition of PSNR is the same except that the MSE is the sum over all \n",
    "squared value differences (now for each color, i.e. three times as many differences as in a monochrome image) divided by \n",
    "image size and by three. Alternately, for color images the image is converted to a different color space and PSNR is reported \n",
    "against each channel of that color space, e.g., YCbCr or HSL.\n",
    "PSNR is most commonly used to measure the quality of reconstruction of lossy compression codecs (e.g., for image compression). \n",
    "The signal in this case is the original data, and the noise is the error introduced by compression. When comparing compression \n",
    "codecs, PSNR is an approximation to human perception of reconstruction quality.\n",
    "Typical values for the PSNR in lossy image and video compression are between 30 and 50 dB, provided the bit depth is 8 bits, \n",
    "where higher is better. The processing quality of 12-bit images is considered high when the PSNR value is 60 dB or higher. \n",
    "For 16-bit data typical values for the PSNR are between 60 and 80 dB. Acceptable values for wireless transmission quality loss \n",
    "are considered to be about 20 dB to 25 dB.\n",
    "In the absence of noise, the two images I and K are identical, and thus the MSE is zero. In this case the PSNR is infinite.\n",
    "\"\"\"\n",
    "\n",
    "from torchmetrics.image import PeakSignalNoiseRatio\n",
    "import torch\n",
    "# , reduction=None, dim=[1,2,3], data_range=255\n",
    "psnr = PeakSignalNoiseRatio(compute_with_cache=False).to(torch.device(\"cuda\", 0))\n",
    "\n",
    "imgs_real = imgs_real.to(torch.device(\"cuda\", 0))\n",
    "imgs_fake = imgs_fake.to(torch.device(\"cuda\", 0))\n",
    "# img_real_1_tensor = img_real_1_tensor.to(torch.device(\"cuda\", 0))\n",
    "# img_fake_1_tensor = img_fake_1_tensor.to(torch.device(\"cuda\", 0))\n",
    "\n",
    "psnr(imgs_fake, imgs_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a901e6a7-1527-4000-9ead-16ca74eadfcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1832, device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute STRUCTURAL SIMILARITY INDEX MEASURE\n",
    "\"\"\"\n",
    "The structural similarity index measure (SSIM) is a method for predicting the perceived quality of digital television and \n",
    "cinematic pictures, as well as other kinds of digital images and videos. It is also used for measuring the similarity between \n",
    "two images. The SSIM index is a full reference metric; in other words, the measurement or prediction of image quality is based \n",
    "on an initial uncompressed or distortion-free image as reference.\n",
    "SSIM is a perception-based model that considers image degradation as perceived change in structural information, while also \n",
    "incorporating important perceptual phenomena, including both luminance masking and contrast masking terms. The difference with \n",
    "other techniques such as MSE or PSNR is that these approaches estimate absolute errors. Structural information is the idea \n",
    "that the pixels have strong inter-dependencies especially when they are spatially close. These dependencies carry important \n",
    "information about the structure of the objects in the visual scene. Luminance masking is a phenomenon whereby image distortions\n",
    "(in this context) tend to be less visible in bright regions, while contrast masking is a phenomenon whereby distortions become \n",
    "less visible where there is significant activity or \"texture\" in the image.\n",
    "This system calculates the Structural Similarity Index between 2 given images which is a value between -1 and +1. \n",
    "A value of +1 indicates that the 2 given images are very similar or the same while a value of -1 indicates the 2 given images \n",
    "are very different. Often these values are adjusted to be in the range [0, 1], e.g., in torchmetrics, where the extremes \n",
    "hold the same meaning.\n",
    "\"\"\"\n",
    "\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
    "import torch \n",
    "\n",
    "ssim = StructuralSimilarityIndexMeasure(compute_with_cache=False).to(torch.device(\"cuda\", 0))\n",
    "\n",
    "# imgs_real = imgs_real.to(torch.device(\"cuda\", 0))\n",
    "# imgs_real = imgs_real.type(torch.float32)\n",
    "# imgs_fake = imgs_fake.to(torch.device(\"cuda\", 0))\n",
    "# imgs_fake = imgs_fake.type(torch.float32)\n",
    "img_real_1_tensor = img_real_1_tensor.to(torch.device(\"cuda\", 0))\n",
    "img_real_1_tensor = img_real_1_tensor.type(torch.float32)\n",
    "img_fake_1_tensor = img_fake_1_tensor.to(torch.device(\"cuda\", 0))\n",
    "img_fake_1_tensor = img_fake_1_tensor.type(torch.float32)\n",
    "\n",
    "ssim(img_fake_1_tensor, img_real_1_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e70c34a3-66df-4a31-82d1-f48024946e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute CLIP score  hardly to use\n",
    "\"\"\"\n",
    "CLIP Score is a reference-free metric that can be used to evaluate the correlation between a generated caption for an \n",
    "image and the actual content of the image. It corresponds to the cosine similarity between visual CLIP embedding for an image \n",
    "and textual CLIP embedding for a caption. The score is bound between 0 and 100 and the closer to 100 the better.\n",
    "\"\"\"\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchmetrics.multimodal.clip_score import CLIPScore\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "\n",
    "prompt = \"A bathroom, a white bathtub takes center stage, positioned directly \"\\\n",
    "\"beneath a mirror that stretches across the wall. The room is clad in gleaming white tiles, which reflect the soft light and \"\\\n",
    "\"create a sense of calm. A glass shower door adorns the bathtub. A few plush towels \"\\\n",
    "\"are carefully laid out nearby, with one draped casually over the side of the tub. \"\n",
    "# To complete the tranquil atmosphere, a mirror stands adjacent to the tub, its reflective surface bouncing light and creating the illusion of a more spacious area\n",
    "\n",
    "img = Image.open(\"/home/jovyan/DFBench/data/IMAGEs/MSCOCO/selected/000000377132.jpg\")\n",
    "img_tensor = pil_to_tensor(img).type(torch.uint8)\n",
    "img_tensor = torch.unsqueeze(img_tensor, 0).to(torch.device(\"cuda\", 0))\n",
    "\n",
    "metric = CLIPScore(model_name_or_path=\"openai/clip-vit-large-patch14\", \n",
    "                   compute_with_cache=False).to(torch.device(\"cuda\", 0))\n",
    "\n",
    "score = metric(img_tensor, prompt)\n",
    "score.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fd5928d-f684-4e45-8bb7-fbc662e8aa3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4596, device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,3\"  # specify which GPU(s) to be used\n",
    "# inside the code:\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device(\"cuda:1,3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import image_similarity_metrics as ism\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "ism.compute_SSIM(imgs_real, imgs_fake, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e6d612-cedc-42b9-b9df-d004e90e5dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import image_similarity_metrics as ism\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "\n",
    "prompt = \"A bathroom, a white bathtub takes center stage, positioned directly \"\\\n",
    "\"beneath a mirror that stretches across the wall. The room is clad in gleaming white tiles, which reflect the soft light and \"\\\n",
    "\"create a sense of calm. A glass shower door adorns the bathtub. A few plush towels \"\\\n",
    "\"are carefully laid out nearby, with one draped casually over the side of the tub. \"\n",
    "# To complete the tranquil atmosphere, a mirror stands adjacent to the tub, its reflective surface bouncing light and creating the illusion of a more spacious area\n",
    "\n",
    "img = Image.open(\"/home/jovyan/DFBench/data/IMAGEs/MSCOCO/selected/000000377132.jpg\")\n",
    "img_tensor = pil_to_tensor(img).type(torch.uint8)\n",
    "img_tensor = torch.unsqueeze(img_tensor, 0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "ism.compute_CLIP_SCORE(prompt, img_tensor, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc503952-552d-4ffd-9312-4c5271b7a0e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
