{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c5d1d8-0afb-41ac-bb65-46fe823f5362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import builtins\n",
    "\n",
    "code_dir_root = \"/home/jovyan/3MDBench/ds_processors/\"\n",
    "sys.path.insert(0, code_dir_root)\n",
    "builtins.CODE_DIR_ROOT_ = code_dir_root\n",
    "img_path_root = \"/home/jovyan/3MDBench/data/IMAGEs/\"\n",
    "ds_path = os.path.join(img_path_root, \"MSCOCO\")\n",
    "output_path_root = os.path.join(img_path_root, \"generated\")\n",
    "import generation_script as gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48276a0c-1416-40d3-bf4f-aafcb6b595a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sel_cap_file_path = os.path.join(ds_path, \"sel_captions_5000.txt\")\n",
    "img_cap_file_path = os.path.join(ds_path, \"image_caption_dict.txt\")\n",
    "captions_list_str = \"\"\n",
    "with open(sel_cap_file_path, \"r\") as sel_cap_file, open(img_cap_file_path, \"r\") as img_cap_file:\n",
    "    sel_cap_dict = json.load(sel_cap_file)\n",
    "    img_cap_dict = json.load(img_cap_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87cbe93c-bd1f-4a2f-ad79-c9c427c577fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load raw_captions  import ast\n",
    "\n",
    "prompt_path = os.path.join(ds_path, \"generated\")\n",
    "raw_img_cap_file_path = os.path.join(prompt_path, \"raw_img_caption_5000.txt\")\n",
    "# json_string = \"\"\n",
    "# raw_img_cap_dict = dict()\n",
    "\n",
    "# with open(raw_img_cap_file_path, \"r\") as raw_img_cap_file:\n",
    "#     for line in raw_img_cap_file:\n",
    "#         if not line.startswith(\" counter:\"):\n",
    "#             json_string += line\n",
    "#         else:\n",
    "#             if len(raw_img_cap_dict) == 0:\n",
    "#                 raw_img_cap_dict = json.loads(json_string)\n",
    "#             else:\n",
    "#                 raw_img_cap_dict |= json.loads(json_string)\n",
    "#             json_string = \"\"\n",
    "raw_img_cap_dict = gs.load_img_captions_dict(raw_img_cap_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "214d4486-e55e-4f2d-811a-012ac7a63438",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load para_captions \n",
    "\n",
    "import json\n",
    "prompt_path = os.path.join(ds_path, \"generated\")\n",
    "para_img_cap_file_path = os.path.join(prompt_path, \"para_img_caption_5000.txt\")\n",
    "para_img_cap_dict = gs.load_img_captions_dict(para_img_cap_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a07d6ba6-eb26-4c31-a3ce-418f0fa7cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### convert captioner_captions\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def get_img_cap_list(captioner_img_cap_file_path):\n",
    "    json_string = \"\"\n",
    "    img_cap_list = []\n",
    "    with open(captioner_img_cap_file_path, \"r\") as captioner_img_cap_file:\n",
    "        for line in captioner_img_cap_file:\n",
    "            if not line.startswith(\" counter:\"):\n",
    "                json_string += line\n",
    "            else:\n",
    "                if len(img_cap_list) == 0:\n",
    "                    img_cap_list = json.loads(json_string)\n",
    "                else:\n",
    "                    img_cap_list = [*img_cap_list, *(json.loads(json_string))]\n",
    "                json_string = \"\"\n",
    "    img_cap_arr = np.array(img_cap_list)\n",
    "    img_cap_list = img_cap_arr.squeeze()\n",
    "    \n",
    "    return img_cap_list\n",
    "\n",
    "prompt_path = os.path.join(ds_path, \"generated\")\n",
    "img_cap_lists = np.array([])\n",
    "for idx in range(1, 11, 1):\n",
    "    captioner_img_cap_file_path = os.path.join(prompt_path, f\"captioner_img_caption_5000_{idx}.txt\")\n",
    "    if img_cap_lists.shape[0] == 0:\n",
    "        img_cap_lists = get_img_cap_list(captioner_img_cap_file_path)\n",
    "    else:\n",
    "        img_cap_list = get_img_cap_list(captioner_img_cap_file_path)\n",
    "        img_cap_lists = np.vstack((img_cap_lists, img_cap_list))\n",
    "captioner_img_caption_dict = dict()\n",
    "img_ids = list(sel_cap_dict.keys())\n",
    "for img_index in range(img_cap_lists.shape[1]):\n",
    "    img_cap_arr = img_cap_lists[:, img_index]\n",
    "    img_cap_arr = np.array([img_cap[1:] for img_cap in img_cap_arr], dtype=img_cap_arr.dtype)\n",
    "    captioner_img_caption_dict[img_ids[img_index]] = img_cap_arr.tolist()\n",
    "with open(os.path.join(prompt_path, \"captioner_img_caption_5000.txt\"), \"w\") as captioner_img_caption_file:\n",
    "    json.dump(captioner_img_caption_dict, captioner_img_caption_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14d55c90-c018-43ed-be28-a3fb47772ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load captioner_captions \n",
    "\n",
    "import json\n",
    "prompt_path = os.path.join(ds_path, \"generated\")\n",
    "captioner_img_cap_file_path = os.path.join(prompt_path, \"captioner_img_caption_5000.txt\")\n",
    "captioner_img_cap_dict = gs.load_cap_img_captions_dict(captioner_img_cap_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1471ecdf-253a-4f71-aaf5-3ce955d758cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load kandinsky 3 pipeline\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "repo_id = \"kandinsky-community/kandinsky-3\"\n",
    "variant= \"fp16\"\n",
    "cache_dir = os.path.join(code_dir_root, \"image_generators\", \"Kandinsky3\", \"cache\")\n",
    "t2i_or_i2i = False\n",
    "pipe = gs.load_img_generator_pipeline(repo_id, variant, cache_dir, t2i_or_i2i, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97332c9-7009-4747-8676-82306cfe1e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load PixArt_Σ pipeline\n",
    "\n",
    "import torch\n",
    "\n",
    "repo_id = \"PixArt-alpha/PixArt-Sigma-XL-2-1024-MS\"\n",
    "variant = \"\"\n",
    "cache_dir = os.path.join(code_dir_root, \"image_generators\", \"PixArt_Σ\", \"cache\")\n",
    "t2i_or_i2i = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch_compile = False\n",
    "pipe = gs.load_img_generator_pipeline(repo_id, variant, cache_dir, t2i_or_i2i, device, torch_compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aff4e52-8bba-4671-b18d-046675736385",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load StableDiffusion3 pipeline\n",
    "\n",
    "import torch\n",
    "\n",
    "repo_id = \"stabilityai/stable-diffusion-3-medium-diffusers\"\n",
    "variant = \"\"\n",
    "cache_dir = os.path.join(code_dir_root, \"image_generators\", \"StableDiffusion3\", \"cache\")\n",
    "t2i_or_i2i = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch_compile = False\n",
    "pipe = gs.load_img_generator_pipeline(repo_id, variant, cache_dir, t2i_or_i2i, device, torch_compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618cd20c-ce90-4934-aa91-8689e75e0708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_processors import image_similarity_metrics as ism\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "from image_generators.Kandinsky3 import generator as gen\n",
    "import torch\n",
    "\n",
    "size = (512, 512)\n",
    "num_inference_steps = 50\n",
    "img_width = 512\n",
    "img_height = 512\n",
    "count = 1\n",
    "images = None\n",
    "img_id = 0\n",
    "output_path = os.path.join(output_path_root, \"Kandinsky3\", \"MSCOCO\", \"raw_prompt\")\n",
    "for raw_key, raw_caps in raw_img_cap_dict.items():\n",
    "    img_id = raw_key\n",
    "    prompts = [*raw_caps, *raw_caps]\n",
    "    if count < 2:\n",
    "        real_img_path = os.path.join(ds_path, \"selected\", img_cap_dict[img_id][0])\n",
    "        # images = gen.run_kandinsky3_t2i(pipe, num_inference_steps, img_width, img_height, prompts, \n",
    "                                        # manual_seed=False, seed=None)\n",
    "        images = gen.run_kandinsky3_i2i(pipe, num_inference_steps, img_width, img_height, prompts, \n",
    "                                real_img_path, manual_seed=False, seed=None)\n",
    "        \n",
    "        real_img_tensor = gs.convert_img_to_imgtensor(real_img_path, None, size)\n",
    "        similarities = torch.tensor([], dtype=torch.float32, device=device)\n",
    "        for fake_img in images:\n",
    "            fake_img_tensor = gs.convert_img_to_imgtensor(None, fake_img, size)\n",
    "            similarity_metrics = torch.tensor([[\n",
    "                                  ism.compute_LPIPS(real_img_tensor, fake_img_tensor, 'squeeze', \n",
    "                                                    device), \n",
    "                                  ism.compute_PSNR(real_img_tensor, fake_img_tensor, device),\n",
    "                                  ism.compute_SSIM(real_img_tensor, fake_img_tensor, device)]],\n",
    "                      dtype=torch.float32, device=device)\n",
    "            similarities = torch.cat((similarities, similarity_metrics), 0)\n",
    "        print(f\"similarities: {similarities}\")\n",
    "        best_indices = torch.tensor([], dtype=torch.int32, device=device)\n",
    "        for col_idx in range(similarities.size(dim=1)):\n",
    "            if col_idx == 0:\n",
    "                best_indices = torch.cat((best_indices, similarities[:,col_idx].min(dim=0)[1].unsqueeze(dim=0)), 0)\n",
    "            else:\n",
    "                best_indices = torch.cat((best_indices, similarities[:,col_idx].max(dim=0)[1].unsqueeze(dim=0)), 0)\n",
    "        print(f\"best_indices: {best_indices}\")\n",
    "        unique_counts = best_indices.unique(return_counts=True)[1]\n",
    "        if (unique_counts == 1).all():\n",
    "            unique_idx = torch.randint(0, unique_counts.size(dim=0), (1,))\n",
    "            print(f\"all 1s, unique_idx: {unique_idx}\")\n",
    "        else:\n",
    "            unique_idx = unique_counts.max(dim=0)[1]\n",
    "            print(f\"unique max, unique_idx: {unique_idx}\")\n",
    "        print(f\"unique_counts: {unique_counts}\")\n",
    "        result_img = images[best_indices[unique_idx]]\n",
    "    else:\n",
    "        break\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e28b31db-6916-4a20-bedb-ca660e3d576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(output_path_root, \"Kandinsky3\", \"MSCOCO\", \"raw_prompt\")\n",
    "output_file_path = os.path.join(output_path, \"SSIM\", img_cap_dict[img_id][0])\n",
    "images[6].save(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd220faa-ae7c-45f8-8cef-1bb01bd735d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "ds_path_root = \"/home/jovyan/3MDBench/data/IMAGEs/\"\n",
    "real_path = f\"{ds_path_root}MSCOCO/selected/\"\n",
    "fake_path = f\"{ds_path_root}generated/Kandinsky3/MSCOCO/raw_prompt/\"\n",
    "out_path = os.path.join(fake_path, \"REAL\")\n",
    "for _, _, img_files in os.walk(os.path.join(fake_path, \"LPIPS\")):\n",
    "    for img_file in img_files:\n",
    "        print(f\"img_file: {img_file}\")\n",
    "        src_file_path = os.path.join(real_path, img_file)\n",
    "        des_file_path = os.path.join(out_path, img_file)\n",
    "        shutil.copy(src_file_path, des_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b3e20c-b2a5-4a4c-a076-7ed1716f27c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for best_idx in range(best_indices.size(dim=0)):\n",
    "            img_idx = best_indices[best_idx].item()\n",
    "            if best_idx == 0:\n",
    "                output_file_path_1 = os.path.join(output_path, \"LPIPS\", img_cap_dict[img_id][0])\n",
    "                output_file_path_2 = os.path.join(output_path, f\"LPIPS{count}\", img_cap_dict[img_id][0])\n",
    "            elif best_idx == 1:\n",
    "                output_file_path_1 = os.path.join(output_path, \"PSNR\", img_cap_dict[img_id][0])\n",
    "                output_file_path_2 = os.path.join(output_path, f\"PSNR{count}\", img_cap_dict[img_id][0])\n",
    "            elif best_idx == 2:\n",
    "                output_file_path_1 = os.path.join(output_path, \"SSIM\", img_cap_dict[img_id][0])\n",
    "                output_file_path_2 = os.path.join(output_path, f\"SSIM{count}\", img_cap_dict[img_id][0])\n",
    "            images[img_idx].save(output_file_path_1)\n",
    "            images[img_idx].save(output_file_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fd5558e-da6d-45c4-a945-bd5018caf018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[[2,5,5], [3,6,9], [8,8,6]], [[2,3,5], [1,6,9], [2,1,1]]])\n",
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef1893ae-d2ce-42f5-932e-b0f9fb818db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 9])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.repeat_interleave(a, 3, dim=2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de201a4-f5d8-4495-8d60-fc4d3d871c06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
